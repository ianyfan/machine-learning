{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import gym\n",
    "from logger import Plotter\n",
    "import numpy as np\n",
    "from replay import ReplayBuffer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "plotter = Plotter('Return',\n",
    "                  #'Length',\n",
    "                  'Explained Variance',\n",
    "                  'Loss',\n",
    "                  'Epsilon')\n",
    "\n",
    "# create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "\n",
    "BUFFER_SIZE = 10 ** 6\n",
    "BATCH_SIZE = 100\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, obs_dim, 1)\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "def mlp(in_dim, act_dim, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    return nn.Sequential(nn.Linear(in_dim, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, act_dim),\n",
    "                         output_activation())\n",
    "\n",
    "V_MIN = 0\n",
    "V_MAX = 102\n",
    "ATOMS = torch.tensor(range(V_MIN, V_MAX))\n",
    "\n",
    "class EpsGreedyActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.act_dim = act_dim\n",
    "        self.q = mlp(obs_dim, act_dim * len(ATOMS))\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.q(state).reshape(-1, self.act_dim, len(ATOMS))\n",
    "    \n",
    "    def probs(self, state):\n",
    "        logits = self.forward(state)\n",
    "        return nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    def log_probs(self, state):\n",
    "        logits = self.forward(state)\n",
    "        return nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    def act(self, state, epsilon=0):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.act_dim)\n",
    "        q = (self.probs(state) * ATOMS).sum(dim=-1)\n",
    "        return q.max(dim=-1).indices.item()\n",
    "    \n",
    "q_net = EpsGreedyActor(obs_dim, act_dim)\n",
    "optimiser = torch.optim.Adam(q_net.q.parameters(), lr=5e-4)\n",
    "epsilon = 1\n",
    "\n",
    "target_net = EpsGreedyActor(obs_dim, act_dim)\n",
    "for p in target_net.q.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "def update_target_net(source_net, target_net):\n",
    "    with torch.no_grad():\n",
    "        target_net.q.load_state_dict(source_net.q.state_dict())\n",
    "update_target_net(q_net, target_net)\n",
    "\n",
    "def preprocess(observations, actions):\n",
    "    return torch.as_tensor(observations[-1], dtype=torch.float32)\n",
    "\n",
    "last_q_net = EpsGreedyActor(obs_dim, act_dim)\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPISODES = 100000\n",
    "epoch = -1\n",
    "for episode in range(EPISODES):\n",
    "    observations = []\n",
    "    actions = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    observations.append(obs.copy())\n",
    "    state = preprocess(observations, actions)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if epsilon > 0.02:\n",
    "            epsilon -= 1e-6\n",
    "        with torch.no_grad():\n",
    "            action = q_net.act(state, epsilon)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        observations.append(obs.copy())\n",
    "        actions.append(action)\n",
    "\n",
    "        last_state, state = state, preprocess(observations, actions)\n",
    "        transition = (last_state, action, reward, state, done)\n",
    "        replay_buffer.store(transition)\n",
    "\n",
    "        # calculate loss\n",
    "        batch = replay_buffer.sample(BATCH_SIZE)\n",
    "        batch_states, batch_actions, batch_rewards, batch_next_states, batch_terminals = batch\n",
    "        batch_actions = torch.as_tensor(batch_actions, dtype=torch.int64)\n",
    "\n",
    "        log_probs = q_net.log_probs(batch_states).gather(dim=1, index=batch_actions.repeat(1, len(ATOMS)).unsqueeze(1)).squeeze()\n",
    "\n",
    "        next_probs = target_net.probs(batch_next_states)\n",
    "        next_actions = (next_probs * ATOMS).sum(dim=-1).max(dim=-1).indices\n",
    "        next_action_probs = next_probs.gather(dim=1, index=next_actions.unsqueeze(1).repeat(1, len(ATOMS)).unsqueeze(1)).squeeze()\n",
    "\n",
    "        r = batch_rewards.repeat(len(ATOMS), 1).T\n",
    "        t = batch_terminals.repeat(len(ATOMS), 1).T\n",
    "        T_z = (r + (1 - t) * GAMMA * ATOMS).clamp(V_MIN, V_MAX)\n",
    "        b_j = (T_z - V_MIN) / 1\n",
    "        l = b_j.floor().type(torch.int64)\n",
    "        u = l + 1\n",
    "        losses = -next_action_probs * ((u - b_j) * log_probs.gather(dim=1, index=l) +\n",
    "                                       (b_j - l) * log_probs.gather(dim=1, index=u))\n",
    "        loss = losses.sum(dim=1).mean() \n",
    "\n",
    "        update_target_net(q_net, last_q_net)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        update_target_net(q_net, target_net)\n",
    "\n",
    "    if episode % 50:\n",
    "        continue\n",
    "\n",
    "    epoch = episode\n",
    "    \n",
    "    NUM_TEST_EPISODES = 8\n",
    "    lengths = []\n",
    "    returns = []\n",
    "    explained_variances = []\n",
    "    for _ in range(NUM_TEST_EPISODES):\n",
    "        vvalues = []\n",
    "        rewards = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                q = (q_net.probs(torch.as_tensor(obs, dtype=torch.float32)) * ATOMS).sum(dim=-1).max(dim=-1)\n",
    "            vvalues.append(q.values.item())\n",
    "            action = q.indices.item()\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        def cumulative(iter, discount):\n",
    "            c = iter.clone().detach()\n",
    "            for i in reversed(range(len(c) - 1)):\n",
    "                c[i] += discount * c[i + 1]\n",
    "            return c\n",
    "\n",
    "        test_rewards_to_go = cumulative(torch.as_tensor(rewards), GAMMA)\n",
    "        explained_variance = 1 - (np.array(vvalues) - np.array(test_rewards_to_go)).var() / np.array(test_rewards_to_go).var()\n",
    "        explained_variances.append(explained_variance)\n",
    "\n",
    "        returns.append(sum(rewards))\n",
    "\n",
    "    plotter.update(epoch,\n",
    "                   (np.mean(returns), min(returns), max(returns)),\n",
    "                   np.mean(explained_variance),\n",
    "                   loss.item(),\n",
    "                   epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ax = plt.axes(label='b')\n",
    "ax.set_xlim(V_MIN, V_MAX)\n",
    "m = 0\n",
    "\n",
    "obs = env.reset()\n",
    "try:\n",
    "    while True:\n",
    "        env.render()\n",
    "        with torch.no_grad():\n",
    "            probs = target_net.probs(torch.as_tensor(obs, dtype=torch.float32)).squeeze()\n",
    "            action = target_net.act(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            \n",
    "        m = max(m, probs.max())\n",
    "        ax.set_ylim(0, m)\n",
    "        if ax.lines:\n",
    "            for line, d in zip(ax.lines, probs):\n",
    "                line.set_data(ATOMS, d)\n",
    "        else:\n",
    "            for d in probs:\n",
    "                ax.plot(ATOMS, d)\n",
    "        ax.figure.canvas.draw()\n",
    "        \n",
    "        obs, _, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "except (Exception, KeyboardInterrupt) as e:\n",
    "    env.close()\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
