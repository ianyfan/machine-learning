{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from logger import Plotter\n",
    "import gym\n",
    "import numpy as np\n",
    "from replay import ReplayBuffer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "plotter = Plotter('Return', 'Policy Loss', 'Values Loss')\n",
    "\n",
    "# create environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "REPLAY_SIZE = 1000000\n",
    "BATCH_SIZE = 100\n",
    "replay_buffer = ReplayBuffer(REPLAY_SIZE, obs_dim, act_dim)\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "def mlp(in_dim, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    return nn.Sequential(nn.Linear(in_dim, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, act_dim),\n",
    "                         output_activation())\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, lo, hi):\n",
    "        super().__init__()\n",
    "        self.pi = mlp(obs_dim, output_activation=nn.Tanh)\n",
    "        self.lo = lo\n",
    "        self.hi = hi\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.hi[0] * self.pi(obs)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            action = self(obs).numpy()\n",
    "        action += ACT_NOISE * np.random.randn(act_dim)\n",
    "        action = np.clip(action, self.lo, self.hi)\n",
    "        return torch.as_tensor(action, dtype=torch.float32)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.q = mlp(obs_dim + act_dim)\n",
    "\n",
    "    def forward(self, obs, acts):\n",
    "        return self.q(torch.cat((obs, acts), dim=1)).squeeze(dim=1)\n",
    "\n",
    "policy_net = Actor(obs_dim, env.action_space.low, env.action_space.high)\n",
    "values_net = Critic(obs_dim, act_dim)\n",
    "\n",
    "PI_LR = 1e-3\n",
    "V_LR = 1e-3\n",
    "policy_optimiser = torch.optim.Adam(policy_net.parameters(), lr=PI_LR)\n",
    "values_optimiser = torch.optim.Adam(values_net.parameters(), lr=V_LR)\n",
    "\n",
    "target_policy_net = Actor(obs_dim, env.action_space.low, env.action_space.high)\n",
    "target_values_net = Critic(obs_dim, act_dim)\n",
    "for p in target_policy_net.pi.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in target_values_net.q.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def update_target_nets(polyak):\n",
    "    with torch.no_grad():\n",
    "        for net, target_net in [[policy_net.pi, target_policy_net.pi], [values_net.q, target_values_net.q]]:\n",
    "            for p, p_targ in zip(net.parameters(), target_net.parameters()):\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "update_target_nets(0)\n",
    "\n",
    "ACT_NOISE = 0.2\n",
    "GAMMA = 0.99\n",
    "POLYAK = 0.995\n",
    "EPOCHS = 1000\n",
    "STEPS_PER_EPOCH = 1000\n",
    "START_STEPS = 10000\n",
    "UPDATE_AFTER = 1000\n",
    "UPDATE_EVERY = 50\n",
    "steps = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    obs = env.reset()\n",
    "    obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "    for _ in range(STEPS_PER_EPOCH):\n",
    "        steps += 1\n",
    "\n",
    "        if steps > START_STEPS:\n",
    "            action = policy_net.act(obs)\n",
    "        else:\n",
    "            action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_obs = torch.as_tensor(next_obs, dtype=torch.float32)\n",
    "\n",
    "        replay_buffer.store((obs, action, reward, next_obs, done))\n",
    "\n",
    "        obs = torch.as_tensor(env.reset(), dtype=torch.float32) if done else next_obs\n",
    "\n",
    "        if steps < UPDATE_AFTER or steps % UPDATE_EVERY:\n",
    "            continue\n",
    "\n",
    "        for _ in range(UPDATE_EVERY):\n",
    "            states, actions, rewards, next_states, terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            y = rewards + GAMMA * (1 - terminals) * target_values_net(next_states, target_policy_net(next_states))\n",
    "            values_loss = ((values_net(states, actions) - y) ** 2).mean()\n",
    "            values_optimiser.zero_grad()\n",
    "            values_loss.backward()\n",
    "            values_optimiser.step()\n",
    "\n",
    "            # optimise: freeze Q net\n",
    "            for p in values_net.q.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            policy_loss = -values_net(states, policy_net(states)).mean()\n",
    "            policy_optimiser.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimiser.step()\n",
    "\n",
    "            # optimise: (unfreeze Q net)\n",
    "            for p in values_net.q.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            update_target_nets(POLYAK)\n",
    "\n",
    "    NUM_TEST_EPISODES = 4\n",
    "    test_returns = []\n",
    "    for _ in range(NUM_TEST_EPISODES):\n",
    "        test_return = 0\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            test_return += reward\n",
    "        test_returns.append(test_return)\n",
    "    plotter.update(epoch,\n",
    "                   (np.mean(test_returns), min(test_returns), max(test_returns)),\n",
    "                   policy_loss.item(),\n",
    "                   values_loss.item())\n",
    "    # print(epoch, np.mean(test_returns), values_loss, policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "try:\n",
    "    while True:\n",
    "        env.render()\n",
    "        with torch.no_grad():\n",
    "            action = policy_net(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, _, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "except (Exception, KeyboardInterrupt):\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD3\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from logger import Plotter\n",
    "import gym\n",
    "import numpy as np\n",
    "from replay import ReplayBuffer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "plotter = Plotter('Return', 'Explained Variance', 'Policy Loss', 'Values Loss')\n",
    "\n",
    "# create environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "REPLAY_SIZE = 1000000\n",
    "BATCH_SIZE = 100\n",
    "replay_buffer = ReplayBuffer(REPLAY_SIZE, obs_dim, act_dim)\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "def mlp(in_dim, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    return nn.Sequential(nn.Linear(in_dim, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, act_dim),\n",
    "                         output_activation())\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, lo, hi):\n",
    "        super().__init__()\n",
    "        self.pi = mlp(obs_dim, output_activation=nn.Tanh)\n",
    "        self.lo = lo\n",
    "        self.hi = hi\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.hi[0] * self.pi(obs)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            action = self(obs).numpy()\n",
    "        action += ACT_NOISE * np.random.randn(act_dim)\n",
    "        action = np.clip(action, self.lo, self.hi)\n",
    "        return torch.as_tensor(action, dtype=torch.float32)\n",
    "    \n",
    "    def target_act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            action = self(obs).numpy()\n",
    "        TARGET_NOISE = 0.1\n",
    "        NOISE_CLIP = 0.2\n",
    "        action += np.clip(TARGET_NOISE * np.random.randn(*action.shape), -NOISE_CLIP, NOISE_CLIP)\n",
    "        action = np.clip(action, self.lo, self.hi)\n",
    "        return torch.as_tensor(action, dtype=torch.float32)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.q = mlp(obs_dim + act_dim)\n",
    "\n",
    "    def forward(self, obs, acts):\n",
    "        return self.q(torch.cat((obs, acts), dim=-1)).squeeze(dim=-1)\n",
    "\n",
    "policy_net = Actor(obs_dim, env.action_space.low, env.action_space.high)\n",
    "values_net_1 = Critic(obs_dim, act_dim)\n",
    "values_net_2 = Critic(obs_dim, act_dim)\n",
    "\n",
    "PI_LR = 1e-3\n",
    "V_LR = 1e-3\n",
    "policy_optimiser = torch.optim.Adam(policy_net.parameters(), lr=PI_LR)\n",
    "values_optimiser_1 = torch.optim.Adam(values_net_1.parameters(), lr=V_LR)\n",
    "values_optimiser_2 = torch.optim.Adam(values_net_2.parameters(), lr=V_LR)\n",
    "\n",
    "target_policy_net = Actor(obs_dim, env.action_space.low, env.action_space.high)\n",
    "target_values_net_1 = Critic(obs_dim, act_dim)\n",
    "target_values_net_2 = Critic(obs_dim, act_dim)\n",
    "for p in target_policy_net.pi.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in target_values_net_1.q.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in target_values_net_2.q.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def update_target_nets(polyak):\n",
    "    with torch.no_grad():\n",
    "        for net, target_net in [(policy_net.pi, target_policy_net.pi),\n",
    "                                (values_net_1.q, target_values_net_1.q),\n",
    "                                (values_net_2.q, target_values_net_2.q)]:\n",
    "            for p, p_targ in zip(net.parameters(), target_net.parameters()):\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "update_target_nets(0)\n",
    "\n",
    "ACT_NOISE = 0.1\n",
    "GAMMA = 0.99\n",
    "POLYAK = 0.995\n",
    "EPOCHS = 1000\n",
    "STEPS_PER_EPOCH = 4000\n",
    "START_STEPS = 10000\n",
    "UPDATE_AFTER = 1000\n",
    "UPDATE_EVERY = 50\n",
    "POLICY_DELAY = 2\n",
    "steps = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    obs = env.reset()\n",
    "    obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "    for _ in range(STEPS_PER_EPOCH):\n",
    "        steps += 1\n",
    "\n",
    "        if steps > START_STEPS:\n",
    "            action = policy_net.act(obs)\n",
    "        else:\n",
    "            action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_obs = torch.as_tensor(next_obs, dtype=torch.float32)\n",
    "\n",
    "        replay_buffer.store((obs, action, reward, next_obs, done))\n",
    "\n",
    "        obs = torch.as_tensor(env.reset(), dtype=torch.float32) if done else next_obs\n",
    "\n",
    "        if steps < UPDATE_AFTER or steps % UPDATE_EVERY:\n",
    "            continue\n",
    "\n",
    "        for i in range(UPDATE_EVERY):\n",
    "            states, actions, rewards, next_states, terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            next_actions = target_policy_net.target_act(next_states)\n",
    "            \n",
    "            y = rewards + GAMMA * (1 - terminals) * torch.min(target_values_net_1(next_states, next_actions),\n",
    "                                                              target_values_net_2(next_states, next_actions))\n",
    "            \n",
    "            values_loss = torch.min(((y - values_net_1(states, actions)) ** 2).mean(),\n",
    "                                    ((y - values_net_2(states, actions)) ** 2).mean())\n",
    "\n",
    "            values_optimiser_1.zero_grad()\n",
    "            values_optimiser_2.zero_grad()\n",
    "            values_loss.backward()\n",
    "            values_optimiser_1.step()\n",
    "            values_optimiser_2.step()\n",
    "            \n",
    "            if (i+1) % POLICY_DELAY:\n",
    "                # optimise: freeze Q net\n",
    "                for p in values_net_1.q.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "                policy_loss = -values_net_1(states, policy_net(states)).mean()\n",
    "                policy_optimiser.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                policy_optimiser.step()\n",
    "\n",
    "                # optimise: unfreeze Q net\n",
    "                for p in values_net_1.q.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "                update_target_nets(POLYAK)\n",
    "\n",
    "    NUM_TEST_EPISODES = 8\n",
    "    lengths = []\n",
    "    returns = []\n",
    "    explained_variances = []\n",
    "    for _ in range(NUM_TEST_EPISODES):\n",
    "        vvalues = []\n",
    "        rewards = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(obs)\n",
    "                vvalues.append(values_net_1(obs, action).squeeze(dim=-1))\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        def cumulative(iter, discount):\n",
    "            c = iter.clone().detach()\n",
    "            for i in reversed(range(len(c) - 1)):\n",
    "                c[i] += discount * c[i + 1]\n",
    "            return c\n",
    "\n",
    "        test_rewards_to_go = cumulative(torch.as_tensor(rewards), GAMMA)\n",
    "        explained_variance = 1 - (np.array(vvalues) - np.array(test_rewards_to_go)).var() / np.array(test_rewards_to_go).var()\n",
    "        explained_variances.append(explained_variance)\n",
    "\n",
    "        returns.append(sum(rewards))\n",
    "\n",
    "    plotter.update(epoch,\n",
    "                   (np.mean(returns), min(returns), max(returns)),\n",
    "                   np.mean(explained_variance),\n",
    "                   policy_loss.item(),\n",
    "                   values_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
