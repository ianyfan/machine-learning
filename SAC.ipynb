{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import gym\n",
    "from logger import Plotter\n",
    "import numpy as np\n",
    "from replay import ReplayBuffer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "plotter = Plotter('Return', 'Explained Variance', 'Policy Loss', 'Values Loss')\n",
    "\n",
    "# create environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "REPLAY_SIZE = 1000000\n",
    "BATCH_SIZE = 100\n",
    "replay_buffer = ReplayBuffer(REPLAY_SIZE, obs_dim, act_dim)\n",
    "\n",
    "HIDDEN_SIZE = 32\n",
    "def mlp(in_dim, out_dim, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    return nn.Sequential(nn.Linear(in_dim, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "                         activation(),\n",
    "                         nn.Linear(HIDDEN_SIZE, out_dim),\n",
    "                         output_activation())\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, lo, hi):\n",
    "        super().__init__()\n",
    "        self.pi = mlp(obs_dim, 2 * act_dim)\n",
    "        self.hi = hi\n",
    "\n",
    "    def act(self, batch, noise=True):\n",
    "        if batch.dim() == 1:\n",
    "            batch = batch.unsqueeze(0)\n",
    "        output = self.pi(batch)\n",
    "        means = output[:, ::2]\n",
    "        if not noise:\n",
    "            return means, 1\n",
    "\n",
    "        std_devs = torch.exp(output[:, 1::2])\n",
    "        gaussian = torch.distributions.Normal(means, std_devs)\n",
    "        u = gaussian.rsample()\n",
    "        actions = torch.tanh(u)\n",
    "        logp = gaussian.log_prob(u) - torch.log(1 - actions ** 2)\n",
    "        return self.hi * actions, logp.squeeze()\n",
    "\n",
    "    def log_prob(self, states, actions):\n",
    "        means = self(states)\n",
    "        noise = actions - means\n",
    "        return self.noise.log_prob(noise)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.q = mlp(obs_dim + act_dim, 1)\n",
    "\n",
    "    def forward(self, obs, acts):\n",
    "        return self.q(torch.cat((obs, acts), dim=1)).squeeze(dim=1)\n",
    "\n",
    "policy_net = Actor(obs_dim, act_dim, env.action_space.low.item(), env.action_space.high.item())\n",
    "q_net_1 = Critic(obs_dim, act_dim)\n",
    "q_net_2 = Critic(obs_dim, act_dim)\n",
    "# values_net = mlp(obs_dim, 1)\n",
    "\n",
    "target_q_net_1 = Critic(obs_dim, act_dim)\n",
    "target_q_net_2 = Critic(obs_dim, act_dim)\n",
    "for net in [target_q_net_1.q, target_q_net_2.q]:\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def update_target_net(coeff):\n",
    "    with torch.no_grad():\n",
    "        for net, target_net in [(q_net_1.q, target_q_net_1.q),\n",
    "                                (q_net_2.q, target_q_net_2.q)]:\n",
    "            for p, p_targ in zip(net.parameters(), target_net.parameters()):\n",
    "                p_targ.data.mul_(1 - coeff)\n",
    "                p_targ.data.add_(coeff * p.data)         \n",
    "update_target_net(1)\n",
    "\n",
    "LR = 3e-4\n",
    "policy_optimiser = torch.optim.Adam(policy_net.parameters(), lr=LR)\n",
    "q_optimiser_1 = torch.optim.Adam(q_net_1.parameters(), lr=LR)\n",
    "q_optimiser_2 = torch.optim.Adam(q_net_2.parameters(), lr=LR)\n",
    "# values_optimiser = torch.optim.Adam(values_net.parameters(), lr=LR)\n",
    "\n",
    "ALPHA = 0.2\n",
    "DISCOUNT = 0.99\n",
    "UPDATE_AFTER = 1000\n",
    "UPDATE_EVERY = 50\n",
    "SMOOTHING_COEFF = 0.005\n",
    "\n",
    "START_STEPS = 1000\n",
    "STEPS_PER_EPOCH = 4000\n",
    "EPOCHS = 100\n",
    "steps = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    done = True\n",
    "    for _ in range(STEPS_PER_EPOCH):\n",
    "        steps += 1\n",
    "\n",
    "        obs = torch.as_tensor(env.reset(), dtype=torch.float32) if done else next_obs\n",
    "        if steps > START_STEPS:\n",
    "            action = policy_net.act(obs)[0].detach()\n",
    "        else:\n",
    "            action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_obs = torch.as_tensor(next_obs, dtype=torch.float32)\n",
    "\n",
    "        replay_buffer.store((obs, action, reward, next_obs, done))\n",
    "\n",
    "        if steps < UPDATE_AFTER or steps % UPDATE_EVERY:\n",
    "            continue\n",
    "\n",
    "        for _ in range(UPDATE_EVERY):\n",
    "            states, actions, rewards, next_states, terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sample_next_actions, sample_next_logps = policy_net.act(next_states)\n",
    "\n",
    "            # values = values_net(states)\n",
    "            # values_loss = ((values - (q_net_1(states, sample_actions) - sample_logps).mean())**2).mean()\n",
    "            # values_optimiser.zero_grad()\n",
    "            # values_loss.backwards()\n",
    "            # values_optimiser.step()\n",
    "\n",
    "            q_1 = target_q_net_1(next_states, sample_next_actions)\n",
    "            q_2 = target_q_net_2(next_states, sample_next_actions)\n",
    "            y = rewards + DISCOUNT * (1 - terminals) * (torch.min(q_1, q_2) - ALPHA * sample_next_logps)\n",
    "\n",
    "            q_loss_1 = ((y - q_net_1(states, actions)) ** 2).mean()\n",
    "            q_optimiser_1.zero_grad()\n",
    "            q_loss_1.backward()\n",
    "            q_optimiser_1.step()\n",
    "\n",
    "            q_loss_2 = ((y - q_net_2(states, actions)) ** 2).mean()\n",
    "            q_optimiser_2.zero_grad()\n",
    "            q_loss_2.backward()\n",
    "            q_optimiser_2.step()\n",
    "\n",
    "            sample_actions, sample_logps = policy_net.act(states)\n",
    "\n",
    "            q1 = q_net_1(states, sample_actions)\n",
    "            q2 = q_net_2(states, sample_actions)\n",
    "\n",
    "            policy_loss = (ALPHA * sample_logps - torch.min(q1, q2)).mean()\n",
    "            policy_optimiser.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimiser.step()\n",
    "\n",
    "            update_target_net(SMOOTHING_COEFF)\n",
    "\n",
    "    NUM_TEST_EPISODES = 8\n",
    "    lengths = []\n",
    "    returns = []\n",
    "    explained_variances = []\n",
    "    for i in range(NUM_TEST_EPISODES):\n",
    "        vvalues = []\n",
    "        rrewards = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                action, _ = policy_net.act(obs, noise=False)\n",
    "                vvalues.append(q_net_1(obs.unsqueeze(0), action).squeeze(dim=-1))\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            rrewards.append(reward)\n",
    "\n",
    "        def cumulative(iter, discount):\n",
    "            c = iter.clone().detach()\n",
    "            for i in reversed(range(len(c) - 1)):\n",
    "                c[i] += discount * c[i + 1]\n",
    "            return c\n",
    "\n",
    "        test_rewards_to_go = cumulative(torch.as_tensor(rrewards), DISCOUNT)\n",
    "        explained_variance = 1 - (np.array(vvalues) - np.array(test_rewards_to_go)).var() / np.array(test_rewards_to_go).var()\n",
    "        explained_variances.append(explained_variance)\n",
    "\n",
    "        returns.append(sum(rrewards))\n",
    "\n",
    "    plotter.update(epoch,\n",
    "                   (np.mean(returns), min(returns), max(returns)),\n",
    "                   np.mean(explained_variance),\n",
    "                   policy_loss.item(),\n",
    "                   (q_loss_1.item(), q_loss_2.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "try:\n",
    "    while True:\n",
    "        env.render()\n",
    "        with torch.no_grad():\n",
    "            action, _ = policy_net.act(torch.as_tensor(obs, dtype=torch.float32), noise=False)\n",
    "        obs, _, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "except (Exception, KeyboardInterrupt):\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
