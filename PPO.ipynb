{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from logger import Plotter\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "plotter = Plotter('Return', 'Length', 'Entropy', 'Explained Variance', 'Policy Loss', 'Values Loss', figsize=(16, 20))\n",
    "\n",
    "HIDDEN_SIZES = (64, 64)\n",
    "def mlp(in_dim, out_dim, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    hidden_layers = [nn.Linear(s1, s2) for s1, s2 in zip(HIDDEN_SIZES[:-1], HIDDEN_SIZES[1:])]\n",
    "    return nn.Sequential(nn.Linear(in_dim, HIDDEN_SIZES[0]),\n",
    "                         activation(),\n",
    "                         *[l for layer in hidden_layers for l in (layer, activation())],\n",
    "                         nn.Linear(HIDDEN_SIZES[-1], out_dim),\n",
    "                         output_activation())\n",
    "\n",
    "\n",
    "class CategoricalActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.pi = mlp(obs_dim, act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.pi(obs)\n",
    "\n",
    "    def act(self, obs, test=False):\n",
    "        with torch.no_grad():\n",
    "            logits = self.pi(obs)\n",
    "        return torch.distributions.Categorical(logits=logits).sample().item()\n",
    "\n",
    "    def log_prob(self, obs, action):\n",
    "        logits = self.pi(obs)\n",
    "        return torch.distributions.Categorical(logits=logits).log_prob(action.squeeze())\n",
    "\n",
    "class GaussianActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.pi = mlp(obs_dim, act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.pi(obs)\n",
    "\n",
    "    def act(self, obs, test=False):\n",
    "        with torch.no_grad():\n",
    "            means = self.pi(obs)\n",
    "        if test:\n",
    "            return means\n",
    "        return torch.distributions.Normal(means, torch.exp(torch.tensor([-0.5]))).sample()\n",
    "    \n",
    "    def log_prob(self, obs, acts):\n",
    "        means = self.pi(obs)\n",
    "        return torch.distributions.Normal(means, torch.exp(torch.tensor([-0.5]))).log_prob(acts).sum(axis=-1)\n",
    "\n",
    "    \n",
    "def product(iter):\n",
    "    ret = 1\n",
    "    for n in iter:\n",
    "        ret *= n\n",
    "    return ret\n",
    "\n",
    "\n",
    "# create environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "obs_dim = product(env.observation_space.shape)\n",
    "\n",
    "act_dim = env.action_space.n\n",
    "policy_net = CategoricalActor(obs_dim, act_dim)\n",
    "act_dim2 = 1\n",
    "\n",
    "# act_dim = env.action_space.shape[0]\n",
    "# policy_net = GaussianActor(obs_dim, act_dim)\n",
    "\n",
    "values_net = mlp(obs_dim, 1)\n",
    "\n",
    "PI_LR = 3e-4\n",
    "V_LR = 1e-3\n",
    "policy_optimiser = torch.optim.Adam(policy_net.parameters(), lr=PI_LR)\n",
    "values_optimiser = torch.optim.Adam(values_net.parameters(), lr=V_LR)\n",
    "\n",
    "def cumulative(iter, discount):\n",
    "    c = iter.clone().detach()\n",
    "    for i in reversed(range(len(c) - 1)):\n",
    "        c[i] += discount * c[i + 1]\n",
    "    return c\n",
    "\n",
    "STEPS_PER_EPOCH = 4000\n",
    "EPOCHS = 10000\n",
    "GAMMA = 0.99\n",
    "TRAIN_PI_ITERS = 80\n",
    "TRAIN_V_ITERS = 80\n",
    "LAMBDA = 0.97\n",
    "CLIP_RATIO = 0.2\n",
    "TARGET_KL = 0.01\n",
    "\n",
    "steps = 0\n",
    "observations = torch.zeros((STEPS_PER_EPOCH, obs_dim), dtype=torch.float32)\n",
    "values = torch.zeros(STEPS_PER_EPOCH, dtype=torch.float32)\n",
    "actions = torch.zeros((STEPS_PER_EPOCH, act_dim2), dtype=torch.float32)\n",
    "advantages = torch.zeros(STEPS_PER_EPOCH, dtype=torch.float32)\n",
    "rewards_to_go = torch.zeros(STEPS_PER_EPOCH, dtype=torch.float32)\n",
    "for epoch in range(EPOCHS):\n",
    "    rewards = []\n",
    "    obs = env.reset()\n",
    "    ep_start = 0\n",
    "    for i in range(STEPS_PER_EPOCH):\n",
    "        steps += 1\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        observations[i] = obs\n",
    "        with torch.no_grad():\n",
    "            values[i] = values_net(obs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = policy_net.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        actions[i] = action\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done or i == STEPS_PER_EPOCH - 1:\n",
    "            ep_end = ep_start + len(rewards)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                last_val = 0.0 if done else values_net(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            rewards_tensor = torch.as_tensor(rewards + [last_val], dtype=torch.float32)\n",
    "\n",
    "            deltas = rewards_tensor[:-1] + GAMMA * torch.cat((values[ep_start+1:ep_end], torch.tensor([last_val]))) - values[ep_start:ep_end]\n",
    "            adv = cumulative(deltas, GAMMA * LAMBDA)\n",
    "            advantages[ep_start:ep_end] = adv # (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "            rewards_to_go[ep_start:ep_end] = cumulative(rewards_tensor, GAMMA)[:-1]\n",
    "\n",
    "            rewards = []\n",
    "            obs = env.reset()\n",
    "\n",
    "            ep_start = ep_end\n",
    "\n",
    "    with torch.no_grad():\n",
    "        old_logp = policy_net.log_prob(observations, actions)\n",
    "\n",
    "    for _ in range(TRAIN_PI_ITERS):\n",
    "        policy_optimiser.zero_grad()\n",
    "        logp = policy_net.log_prob(observations, actions)\n",
    "        ratio = torch.exp(logp - old_logp)\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - CLIP_RATIO, 1 + CLIP_RATIO)\n",
    "        policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "        policy_loss.backward()\n",
    "        policy_optimiser.step()\n",
    "\n",
    "        approx_kl = (old_logp - logp).mean().item()\n",
    "        \n",
    "        if approx_kl > 1.5 * TARGET_KL:\n",
    "            break\n",
    "\n",
    "    for _ in range(TRAIN_V_ITERS):\n",
    "        values_optimiser.zero_grad()\n",
    "        vvalues = values_net(observations).squeeze(dim=-1)\n",
    "        values_loss = ((vvalues - rewards_to_go) ** 2).mean()\n",
    "        values_loss.backward()\n",
    "        values_optimiser.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        lengths = []\n",
    "        returns = []\n",
    "        entropies = []\n",
    "        explained_variances = []\n",
    "        for _ in range(4):\n",
    "            vvalues = []\n",
    "            rewards = []\n",
    "            ep_len, ep_entropy = 0, 0\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                ep_len += 1\n",
    "\n",
    "                obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "                    vvalues.append(values_net(obs).squeeze(dim=-1))\n",
    "                    action = policy_net.act(obs, True)\n",
    "                    ep_entropy -= policy_net.log_prob(obs, torch.as_tensor(action))\n",
    "\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "            test_rewards_to_go = cumulative(torch.as_tensor(rewards), GAMMA)\n",
    "            explained_variance = 1 - (np.array(vvalues) - np.array(test_rewards_to_go)).var() / np.array(test_rewards_to_go).var()\n",
    "            explained_variances.append(explained_variance)\n",
    "            \n",
    "            lengths.append(ep_len)\n",
    "            returns.append(sum(rewards))\n",
    "            entropies.append(ep_entropy / ep_len)\n",
    "\n",
    "        plotter.update(epoch,\n",
    "                       (np.mean(returns), min(returns), max(returns)),\n",
    "                       (np.mean(lengths), min(lengths), max(lengths)),\n",
    "                       np.mean(entropies),\n",
    "                       np.mean(explained_variance),\n",
    "                       policy_loss.item(),\n",
    "                       values_loss.item())\n",
    "\n",
    "    # print(epoch, np.mean(returns), policy_loss, values_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "l = 0\n",
    "done = False\n",
    "while not done:\n",
    "    l += 1\n",
    "    env.render()\n",
    "    with torch.no_grad():\n",
    "        action = policy_net.act(torch.as_tensor(obs.flatten(), dtype=torch.float32), True)\n",
    "    obs, _, done, _ = env.step(action)\n",
    "env.close()\n",
    "print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
